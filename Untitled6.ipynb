{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyXJN5EbpA7qDgeNZDIY3x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlosviniharo/DEA113-G1/blob/main/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KywyW5TJA_lK",
        "outputId": "6944ab62-0bd6-4b4e-d2be-45de16bce06b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "# nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # Text Classification\n",
        "  from sklearn.feature_extraction.text import CountVectorizer\n",
        "  from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "  # Training data\n",
        "  documents = [\n",
        "      (\"Congratulations! You have won a $1,000 gift card. Click here to claim now!\", \"Spam\"),\n",
        "      (\"Get rich fast! Earn $10,000 per week from home. Sign up today!\", \"Spam\"),\n",
        "      (\"Limited-time offer! Buy one, get one free on all our premium products.\", \"Spam\"),\n",
        "      (\"Dear user, your bank account has been compromised. Verify your details immediately!\", \"Spam\"),\n",
        "      (\"Exclusive deal just for you! Click this link to access the secret discount.\", \"Spam\"),\n",
        "      (\"Hey John, are we still meeting for coffee tomorrow at 10 AM?\", \"Inbox\"),\n",
        "      (\"Your invoice for the recent purchase is attached. Let us know if you have any questions.\", \"Inbox\"),\n",
        "      (\"Reminder: Your doctor's appointment is scheduled for Monday at 3 PM.\", \"Inbox\"),\n",
        "      (\"Dear customer, your recent order has been shipped. Track your package here.\", \"Inbox\"),\n",
        "      (\"Team meeting scheduled for Friday. Please confirm your availability.\", \"Inbox\")\n",
        "  ]\n",
        "\n",
        "  # Prepare features and labels\n",
        "  vectorizer = CountVectorizer()\n",
        "  features = vectorizer.fit_transform([doc[0] for doc in documents])\n",
        "  labels = [doc[1] for doc in documents]\n",
        "\n",
        "  # Train a classifier (Naive Bayes)\n",
        "  classifier = MultinomialNB()\n",
        "  classifier.fit(features, labels)\n",
        "\n",
        "  # Test with a new example\n",
        "  spam_example = vectorizer.transform([\"You has won a price please navegate to this link\"])\n",
        "  inbox_example = vectorizer.transform([\"Dear Carlos, Your grades have been updated please check them on Canvas\"])\n",
        "  prediction_spam = classifier.predict(spam_example)\n",
        "  prediction_inbox = classifier.predict(inbox_example)\n",
        "  print(prediction_spam)\n",
        "  print(prediction_inbox)"
      ],
      "metadata": {
        "id": "4aTfwzrVBDxI",
        "outputId": "5f80e100-a6cb-4015-d288-1fdfadfad691",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Spam']\n",
            "['Inbox']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XB04gRhnBdPu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}