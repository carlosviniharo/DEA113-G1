{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlosviniharo/DEA113-G1/blob/main/Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d67ea91",
      "metadata": {
        "id": "1d67ea91"
      },
      "source": [
        "## Text Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "42ac0d30",
      "metadata": {
        "id": "42ac0d30",
        "outputId": "e7db1a69-bb34-4fdf-b64a-3aa85c7921ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "# nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suMZua3e1Jhc",
        "outputId": "fb6323a0-f029-4c4e-e13e-eb21102cf4d5"
      },
      "id": "suMZua3e1Jhc",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b6fa20be",
      "metadata": {
        "id": "b6fa20be",
        "outputId": "a2735abc-c78f-4375-bb64-13b888ef0bca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'Big', 'Data', 'course', 'in', 'CCTB', '.']\n"
          ]
        }
      ],
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "\n",
        "text = \"This is a Big Data course in CCTB.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph_text = \"This is a Big Data course in CCTB.n This is our second semester. We are loving it.\"\n",
        "sent_tokens = nltk.sent_tokenize(paragraph_text)\n",
        "print(sent_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtGrcY7z4MfV",
        "outputId": "2bac0ec2-e593-475a-bee3-0aaffe7288d4"
      },
      "id": "xtGrcY7z4MfV",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is a Big Data course in CCTB.n This is our second semester.', 'We are loving it.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "33b58e3a",
      "metadata": {
        "id": "33b58e3a",
        "outputId": "79061aac-c309-43ad-8828-fc9400497c12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'This': 1, 'is': 1, 'a': 1, 'Big': 1, 'Data': 1, 'course': 1, 'in': 1, 'CCTB': 1, '.': 1})\n"
          ]
        }
      ],
      "source": [
        "# Counter Tokens\n",
        "from collections import Counter\n",
        "\n",
        "word_counts = Counter(tokens)\n",
        "print(word_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6e2b3da5",
      "metadata": {
        "id": "6e2b3da5",
        "outputId": "7e02b5da-cdd0-42f6-a9a8-606c0c0dc22d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Big', 'Data', 'course', 'CCTB', '.']\n"
          ]
        }
      ],
      "source": [
        "# StopWords (The, an , or ,etc.)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(filtered_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "db2358b1",
      "metadata": {
        "id": "db2358b1",
        "outputId": "59a800e2-9bfa-4ada-dd9a-f01d4153c9b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed words: \n",
            "['thi', 'is', 'a', 'big', 'data', 'cours', 'in', 'cctb', '.']\n",
            "['This', 'is', 'a', 'Big', 'Data', 'course', 'in', 'CCTB', '.']\n"
          ]
        }
      ],
      "source": [
        "# Stemming and Lemitizing (Back to root form)\n",
        "\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "print(\"Stemmed words: \")\n",
        "print(stemmed_words)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "print(lemmatized_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4a6faf94",
      "metadata": {
        "id": "4a6faf94",
        "outputId": "fd8dd0f8-b4cd-4599-b2b8-00cabf2f4813",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.0, 'neu': 0.327, 'pos': 0.673, 'compound': 0.9237}\n"
          ]
        }
      ],
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "text = \"I love this course! This is very exciting and amazing!\"\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = sia.polarity_scores(text)\n",
        "print(sentiment_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "37d396df",
      "metadata": {
        "id": "37d396df",
        "outputId": "3dff633a-7411-4180-8da8-902eb9f494ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.611, 'neu': 0.389, 'pos': 0.0, 'compound': -0.7813}\n"
          ]
        }
      ],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "text = \"I Hate this course! Its very hard and confusing\"\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = sia.polarity_scores(text)\n",
        "print(sentiment_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise:\n",
        "## 1. Take a paragraph.\n",
        "## 1. Tokenize into sentences and words.\n",
        "## 1. Remove stop words.\n",
        "## 1. Perform Stemming and Lemmatization, and print the outputs of both."
      ],
      "metadata": {
        "id": "113IH2SX-CkK"
      },
      "id": "113IH2SX-CkK"
    },
    {
      "cell_type": "code",
      "source": [
        "ex_text = \"NLTK, or Natural Language Toolkit, is a free, open-source Python library that helps process and analyze text. It's used by researchers, developers, and data scientists to build NLP applications. \""
      ],
      "metadata": {
        "id": "-2DQutMF-yj8"
      },
      "id": "-2DQutMF-yj8",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_words = nltk.word_tokenize(ex_text)\n",
        "print(\"Tokens by word\")\n",
        "print(tokens_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwESdr1J_E_5",
        "outputId": "4612af17-2d20-4323-ae04-24bf47b926c5"
      },
      "id": "RwESdr1J_E_5",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens by word\n",
            "['NLTK', ',', 'or', 'Natural', 'Language', 'Toolkit', ',', 'is', 'a', 'free', ',', 'open-source', 'Python', 'library', 'that', 'helps', 'process', 'and', 'analyze', 'text', '.', 'It', \"'s\", 'used', 'by', 'researchers', ',', 'developers', ',', 'and', 'data', 'scientists', 'to', 'build', 'NLP', 'applications', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_sentences = nltk.sent_tokenize(ex_text)\n",
        "print(\"Tokens by sentence\")\n",
        "print(tokens_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xs5RSpFM_GXD",
        "outputId": "5447a8b1-a68d-40e2-832f-2b693b2f4fd1"
      },
      "id": "xs5RSpFM_GXD",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens by sentence\n",
            "['NLTK, or Natural Language Toolkit, is a free, open-source Python library that helps process and analyze text.', \"It's used by researchers, developers, and data scientists to build NLP applications.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# StopWords (The, an , or ,etc.)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens_words if word.lower() not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEL-A21y_kuj",
        "outputId": "aab9a1da-ce82-492a-f5ae-b9f031297b2a"
      },
      "id": "VEL-A21y_kuj",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', ',', 'Natural', 'Language', 'Toolkit', ',', 'free', ',', 'open-source', 'Python', 'library', 'helps', 'process', 'analyze', 'text', '.', \"'s\", 'used', 'researchers', ',', 'developers', ',', 'data', 'scientists', 'build', 'NLP', 'applications', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming and Lemitizing (Back to root form)\n",
        "\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in tokens_words]\n",
        "print(\"Stemmed words: \")\n",
        "print(stemmed_words)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens_words]\n",
        "print(\"Lemmatized words: \")\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJEeuNVoAA51",
        "outputId": "8f7aee45-2cb5-4c13-c1a9-f442c95bb0a1"
      },
      "id": "cJEeuNVoAA51",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed words: \n",
            "['nltk', ',', 'or', 'natur', 'languag', 'toolkit', ',', 'is', 'a', 'free', ',', 'open-sourc', 'python', 'librari', 'that', 'help', 'process', 'and', 'analyz', 'text', '.', 'it', \"'s\", 'use', 'by', 'research', ',', 'develop', ',', 'and', 'data', 'scientist', 'to', 'build', 'nlp', 'applic', '.']\n",
            "Lemmatized words: \n",
            "['NLTK', ',', 'or', 'Natural', 'Language', 'Toolkit', ',', 'is', 'a', 'free', ',', 'open-source', 'Python', 'library', 'that', 'help', 'process', 'and', 'analyze', 'text', '.', 'It', \"'s\", 'used', 'by', 'researcher', ',', 'developer', ',', 'and', 'data', 'scientist', 'to', 'build', 'NLP', 'application', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "awrb0xFzAN7C"
      },
      "id": "awrb0xFzAN7C",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "bb2ca1a3",
      "metadata": {
        "id": "bb2ca1a3",
        "outputId": "e2a2ce49-86bf-43c6-8f79-578db66e67b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['positive', 'negative', 'positive', 'negative']\n",
            "['positive']\n"
          ]
        }
      ],
      "source": [
        "# Text Classification\n",
        "\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Training data\n",
        "documents = [\n",
        "    (\"I love this course\", \"positive\"),\n",
        "    (\"I hate this program\", \"negative\"),\n",
        "    (\"This was an awesome movie\", \"positive\"),\n",
        "    (\"The course was terrible\", \"negative\")\n",
        "]\n",
        "\n",
        "# Prepare features and labels\n",
        "vectorizer = CountVectorizer()\n",
        "features = vectorizer.fit_transform([doc[0] for doc in documents])\n",
        "labels = [doc[1] for doc in documents]\n",
        "print(labels)\n",
        "\n",
        "# Train a classifier (Naive Bayes)\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(features, labels)\n",
        "\n",
        "# Test with a new example\n",
        "new_example = vectorizer.transform([\"I really enjoyed watching this film\"])\n",
        "prediction = classifier.predict(new_example)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "ab0a6c7a",
      "metadata": {
        "id": "ab0a6c7a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}